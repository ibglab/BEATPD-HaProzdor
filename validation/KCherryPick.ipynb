{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cherry pick\n",
    "Generate the test prediction files using cherry picking from ensembles of feature predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(asctime)s: %(message)s')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import getpass\n",
    "\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "logging.info(\"Numpy version \" + np.__version__)\n",
    "import scipy as sp\n",
    "import scipy.signal as sig\n",
    "logging.info(\"Scipy version \" + sp.__version__)\n",
    "import pandas as pd\n",
    "logging.info(\"Pandas version \" + pd.__version__)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "logging.info(\"Matplotlib version \" + matplotlib.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# For the standard seed use 34\n",
    "np.random.seed (34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['cis','real']\n",
    "score_names = ['tremor', 'dyskinesia', 'on_off']\n",
    "\n",
    "labels_folder = 'data'\n",
    "Ks = [0,1,2,3,4]\n",
    "test_rand = 'S34_K'\n",
    "\n",
    "dropNaive = True\n",
    "onlyGood = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BEATPD_loss(df, real_score, est_score):\n",
    "        subject_mse = {}\n",
    "        subject_countsq = {}\n",
    "        subject_mul = {}\n",
    "\n",
    "        for subject in subjects[score]:\n",
    "            idx = (df['subject_id']==subject)\n",
    "            if (~df.loc[idx, real_score].isna().all()):\n",
    "                subject_mse[subject] = ((df.loc[idx, real_score]-df.loc[idx, est_score])**2).mean()\n",
    "                subject_countsq[subject] = np.sqrt(idx.sum())\n",
    "                subject_mul[subject] = subject_mse[subject] * subject_countsq[subject]\n",
    "        loss = sum(subject_mul.values()) / sum(subject_countsq.values())\n",
    "        #logging.info(f'BEATPD {loss:.2f}')\n",
    "        return loss\n",
    "\n",
    "def kloss(df, real_score, est_score):\n",
    "        loss = {}\n",
    "        for subject in subjects[score]:\n",
    "            idx = (df['subject_id']==subject)\n",
    "            if (~df.loc[idx, real_score].isna().all()):\n",
    "                loss[subject] = ((df.loc[idx, real_score]-df.loc[idx, est_score])**2).mean()\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define good and naive subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parms  = { }\n",
    "for K in Ks:\n",
    "    parms [K] = {}\n",
    "    \n",
    "    logging.info(f'K iterator {K}')\n",
    "    training_labels, test_labels, test_data_folder = {}, {}, {}\n",
    "     \n",
    "    for dataset in datasets:\n",
    "        test_label_file = os.path.join(labels_folder, dataset.upper()+f'-PD_Test_Data_IDs_Labels_{test_rand}{K}.csv')\n",
    "        training_label_file = os.path.join(labels_folder, dataset.upper()+f'-PD_Training_Data_IDs_Labels_{test_rand}{K}.csv')\n",
    "\n",
    "        training_labels[dataset] = pd.read_csv(training_label_file)\n",
    "        logging.info(f'Training {dataset}: Read file \"{training_label_file}\" containing {training_labels[dataset].shape[0]} records')\n",
    "\n",
    "        test_labels[dataset] = pd.read_csv(test_label_file)\n",
    "        logging.info(f'Test {dataset}: Read file \"{test_label_file}\" containing {test_labels[dataset].shape[0]} records')\n",
    "        \n",
    "    train_df, test_df = {}, {}\n",
    "    subjects, test_subject_sq = {}, {}\n",
    "    for score in score_names:\n",
    "        field_cols = ['measurement_id', 'subject_id', score]\n",
    "        train_df[score] = pd.concat([training_labels['cis'][field_cols], training_labels['real'][field_cols]]).dropna(subset=[score])\n",
    "        test_df[score] = pd.concat([test_labels['cis'][field_cols], test_labels['real'][field_cols]]).dropna(subset=[score])\n",
    "        subjects[score] = train_df[score]['subject_id'].unique()\n",
    "        logging.info(f'Score: {score}: unique subjects {subjects[score].shape[0]}:\\n{subjects[score]}')\n",
    "        test_subject_sq[score] = [np.sqrt((test_df[score]['subject_id']==subject).sum()) for subject in subjects]\n",
    "        for subject in subjects[score]:\n",
    "            test_df[score].loc[test_df[score]['subject_id']==subject, 'naive_mean'] = train_df[score][train_df[score]['subject_id']==subject][score].mean()\n",
    "            test_df[score].loc[test_df[score]['subject_id']==subject, 'naive_std'] = train_df[score][train_df[score]['subject_id']==subject][score].std()\n",
    "        test_df[score].set_index('measurement_id', inplace=True)\n",
    "\n",
    "    for score in score_names:        \n",
    "        feature_results = sorted(glob.glob(f'features/{test_rand}{K}/*{score}_*.csv'))\n",
    "        logging.info(f'Score {score} - number of feature files {len(feature_results)}')\n",
    "        for i, file_name in enumerate(feature_results):\n",
    "            d = pd.read_csv(file_name).set_index('measurement_id')\n",
    "            test_df[score][f'f{i}'] = d['prediction']\n",
    "            idx = test_df[score][f'f{i}'].isna() & ~test_df[score]['naive_mean'].isna()\n",
    "            test_df[score].loc[idx,f'f{i}'] = test_df[score].loc[idx,'naive_mean']      \n",
    "\n",
    "    ### Generate a single dataframe\n",
    "\n",
    "    for j, score in enumerate(score_names):\n",
    "        feature_cols = test_df[score].filter(regex='^f')\n",
    "        test_df[score]['f_mean'] = feature_cols.mean(axis=1)\n",
    "        test_df[score]['f_std'] = feature_cols.std(axis=1)\n",
    "        test_df[score]['f_median'] = feature_cols.median(axis=1)\n",
    "        for i, subject in enumerate(subjects[score]):\n",
    "            for feature_col in feature_cols:\n",
    "                test_df[score].loc[test_df[score]['subject_id']==subject,'a'+feature_col[1:]] = (\n",
    "                    test_df[score].loc[test_df[score]['subject_id']==subject, feature_col] - \n",
    "                    test_df[score].loc[test_df[score]['subject_id']==subject, feature_col].mean() +\n",
    "                    test_df[score].loc[test_df[score]['subject_id']==subject, 'naive_mean'].mean() )\n",
    "\n",
    "        adjusted_cols = test_df[score].filter(regex='^a')\n",
    "        test_df[score]['a_mean'] = adjusted_cols.mean(axis=1)\n",
    "        test_df[score]['a_std'] = adjusted_cols.std(axis=1)\n",
    "        test_df[score]['a_median'] = adjusted_cols.median(axis=1)\n",
    "\n",
    "    score_df = pd.DataFrame()\n",
    "    for score in score_names:\n",
    "        score_df.loc['naive', score] = BEATPD_loss(test_df[score],score,'naive_mean')\n",
    "        score_df.loc['f_mean', score] = BEATPD_loss(test_df[score],score,'f_mean')\n",
    "        score_df.loc['a_mean', score] = BEATPD_loss(test_df[score],score,'a_mean')\n",
    "        \n",
    "        parms[K][(score, 'score', 'naive')] = kloss(test_df[score],score,'naive_mean')\n",
    "        parms[K][(score, 'score', 'ayala')] = kloss(test_df[score],score,'a_mean')\n",
    "\n",
    "        negfactor=0\n",
    "        posfactor=0\n",
    "        sneg=test_df[score][(test_df[score]['a_mean']+negfactor*test_df[score]['a_std']<test_df[score]['naive_mean'])  ]\n",
    "        spos=test_df[score][(test_df[score]['a_mean']-posfactor*test_df[score]['a_std']>test_df[score]['naive_mean'])  ]        \n",
    "\n",
    "        logging.info(f'Negative {sneg.shape[0]} Positive {spos.shape[0]} of {test_df[score].shape[0]}')\n",
    "        logging.info(f\"Neg {(sneg[score]<sneg['naive_mean']).sum()} of {sneg.shape[0]} {100*(sneg[score]<sneg['naive_mean']).sum()/sneg.shape[0]:.1f}%\")\n",
    "        logging.info(f\"Pos {(spos[score]>spos['naive_mean']).sum()} of {spos.shape[0]} {100*(spos[score]>spos['naive_mean']).sum()/spos.shape[0]:.1f}%\")\n",
    "        test_df[score]['cherry'] = test_df[score]['naive_mean']\n",
    "        #test_df[score].loc[sneg.index, 'cherry'] = sneg['a_mean'] + (sneg['a_mean']-sneg['naive_mean']) * 0.5\n",
    "        #test_df[score].loc[spos.index, 'cherry'] = spos['a_mean'] + (spos['a_mean']-spos['naive_mean']) * 0.5\n",
    "        test_df[score].loc[sneg.index, 'cherry'] = sneg['a_mean'] \n",
    "        test_df[score].loc[spos.index, 'cherry'] = spos['a_mean'] \n",
    "        score_df.loc['cherry', score] = BEATPD_loss(test_df[score],score,'cherry')\n",
    "    parms[K]['score'] = score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = list(training_labels['cis']['subject_id'].unique()) + list(training_labels['real']['subject_id'].unique())\n",
    "da = pd.DataFrame(index=sss)\n",
    "good_df = pd.DataFrame()\n",
    "naive_df = pd.DataFrame()\n",
    "for score in score_names:\n",
    "    for K in Ks:\n",
    "        da[score+str(K)] = pd.Series(parms[K][(score,'score','naive')]) - pd.Series(parms[K][(score,'score','ayala')])  \n",
    "    dd = da.filter(regex=f'{score}')\n",
    "    good_df[score] = dd.mean(axis=1)>0.0\n",
    "    naive_df[score] = dd.mean(axis=1)<0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in score_names:\n",
    "    print(score)\n",
    "    print(naive_df[naive_df[score]].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read label files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kscore = {}\n",
    "parms  = { }\n",
    "for K in Ks:\n",
    "    parms [K] = {}\n",
    "    \n",
    "    logging.info(f'K iterator {K}')\n",
    "    training_labels, test_labels, test_data_folder = {}, {}, {}\n",
    "     \n",
    "    for dataset in datasets:\n",
    "        labels_folder = 'data'\n",
    "        test_label_file = os.path.join(labels_folder, dataset.upper()+f'-PD_Test_Data_IDs_Labels_{test_rand}{K}.csv')\n",
    "        training_label_file = os.path.join(labels_folder, dataset.upper()+f'-PD_Training_Data_IDs_Labels_{test_rand}{K}.csv')\n",
    "\n",
    "        training_labels[dataset] = pd.read_csv(training_label_file)\n",
    "        logging.info(f'Training {dataset}: Read file \"{training_label_file}\" containing {training_labels[dataset].shape[0]} records')\n",
    "\n",
    "        test_labels[dataset] = pd.read_csv(test_label_file)\n",
    "        logging.info(f'Test {dataset}: Read file \"{test_label_file}\" containing {test_labels[dataset].shape[0]} records')\n",
    "        \n",
    "    train_df, test_df = {}, {}\n",
    "    subjects, test_subject_sq = {}, {}\n",
    "    for score in score_names:\n",
    "        field_cols = ['measurement_id', 'subject_id', score]\n",
    "        train_df[score] = pd.concat([training_labels['cis'][field_cols], training_labels['real'][field_cols]]).dropna(subset=[score])\n",
    "        test_df[score] = pd.concat([test_labels['cis'][field_cols], test_labels['real'][field_cols]]).dropna(subset=[score])\n",
    "        subjects[score] = train_df[score]['subject_id'].unique()\n",
    "        logging.info(f'Score: {score}: unique subjects {subjects[score].shape[0]}:\\n{subjects[score]}')\n",
    "        test_subject_sq[score] = [np.sqrt((test_df[score]['subject_id']==subject).sum()) for subject in subjects]\n",
    "        for subject in subjects[score]:\n",
    "            test_df[score].loc[test_df[score]['subject_id']==subject, 'naive_mean'] = train_df[score][train_df[score]['subject_id']==subject][score].mean()\n",
    "            test_df[score].loc[test_df[score]['subject_id']==subject, 'naive_std'] = train_df[score][train_df[score]['subject_id']==subject][score].std()\n",
    "        test_df[score].set_index('measurement_id', inplace=True)\n",
    "\n",
    "    for score in score_names:        \n",
    "        feature_results = sorted(glob.glob(f'features/{test_rand}{K}/*{score}_*.csv'))\n",
    "        logging.info(f'Score {score} - number of feature files {len(feature_results)}')\n",
    "        for i, file_name in enumerate(feature_results):\n",
    "            d = pd.read_csv(file_name).set_index('measurement_id')\n",
    "            test_df[score][f'f{i}'] = d['prediction']\n",
    "            idx = test_df[score][f'f{i}'].isna() & ~test_df[score]['naive_mean'].isna()\n",
    "            test_df[score].loc[idx,f'f{i}'] = test_df[score].loc[idx,'naive_mean']      \n",
    "\n",
    "    ### Generate a single dataframe\n",
    "\n",
    "    for j, score in enumerate(score_names):\n",
    "        feature_cols = test_df[score].filter(regex='^f')\n",
    "        test_df[score]['f_mean'] = feature_cols.mean(axis=1)\n",
    "        test_df[score]['f_std'] = feature_cols.std(axis=1)\n",
    "        test_df[score]['f_median'] = feature_cols.median(axis=1)\n",
    "        for i, subject in enumerate(subjects[score]):\n",
    "            for feature_col in feature_cols:\n",
    "                test_df[score].loc[test_df[score]['subject_id']==subject,'a'+feature_col[1:]] = (\n",
    "                    test_df[score].loc[test_df[score]['subject_id']==subject, feature_col] - \n",
    "                    test_df[score].loc[test_df[score]['subject_id']==subject, feature_col].mean() +\n",
    "                    test_df[score].loc[test_df[score]['subject_id']==subject, 'naive_mean'].mean() )\n",
    "\n",
    "        adjusted_cols = test_df[score].filter(regex='^a')\n",
    "        test_df[score]['a_mean'] = adjusted_cols.mean(axis=1)\n",
    "        test_df[score]['a_std'] = adjusted_cols.std(axis=1)\n",
    "        test_df[score]['a_median'] = adjusted_cols.median(axis=1)\n",
    "\n",
    "    score_df = pd.DataFrame()\n",
    "    for score in score_names:\n",
    "        score_df.loc['naive', score] = BEATPD_loss(test_df[score],score,'naive_mean')\n",
    "        score_df.loc['f_mean', score] = BEATPD_loss(test_df[score],score,'f_mean')\n",
    "        score_df.loc['a_mean', score] = BEATPD_loss(test_df[score],score,'a_mean')\n",
    "        \n",
    "        parms[K][(score, 'score', 'naive')] = kloss(test_df[score],score,'naive_mean')\n",
    "        parms[K][(score, 'score', 'ayala')] = kloss(test_df[score],score,'a_mean')\n",
    "\n",
    "        negfactor=0\n",
    "        posfactor=0\n",
    "        sneg=test_df[score][(test_df[score]['a_mean']+negfactor*test_df[score]['a_std']<test_df[score]['naive_mean'])  ]\n",
    "        spos=test_df[score][(test_df[score]['a_mean']-posfactor*test_df[score]['a_std']>test_df[score]['naive_mean'])  ]\n",
    "        \n",
    "        if dropNaive:\n",
    "            naive = naive_df[naive_df[score]].index.tolist()\n",
    "            sneg = sneg[~sneg['subject_id'].isin(naive)]\n",
    "            spos = spos[~spos['subject_id'].isin(naive)]\n",
    "        if onlyGood:\n",
    "            good = good_df[good_df[score]].index.tolist()\n",
    "            sneg = sneg[sneg['subject_id'].isin(good)]\n",
    "            spos = spos[spos['subject_id'].isin(good)]\n",
    "\n",
    "        logging.info(f'Negative {sneg.shape[0]} Positive {spos.shape[0]} of {test_df[score].shape[0]}')\n",
    "        logging.info(f\"Neg {(sneg[score]<sneg['naive_mean']).sum()} of {sneg.shape[0]} {100*(sneg[score]<sneg['naive_mean']).sum()/sneg.shape[0]:.1f}%\")\n",
    "        logging.info(f\"Pos {(spos[score]>spos['naive_mean']).sum()} of {spos.shape[0]} {100*(spos[score]>spos['naive_mean']).sum()/spos.shape[0]:.1f}%\")\n",
    "        test_df[score]['cherry'] = test_df[score]['naive_mean']\n",
    "        #test_df[score].loc[sneg.index, 'cherry'] = sneg['a_mean'] + (sneg['a_mean']-sneg['naive_mean']) * 0.5\n",
    "        #test_df[score].loc[spos.index, 'cherry'] = spos['a_mean'] + (spos['a_mean']-spos['naive_mean']) * 0.5\n",
    "        test_df[score].loc[sneg.index, 'cherry'] = sneg['a_mean'] \n",
    "        test_df[score].loc[spos.index, 'cherry'] = spos['a_mean'] \n",
    "        score_df.loc['cherry', score] = BEATPD_loss(test_df[score],score,'cherry')\n",
    "    parms[K]['score'] = score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for K in Ks:\n",
    "    print(parms[K]['score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
