{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(asctime)s: %(message)s')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel, f_classif, f_regression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "import ClassificationUtils as helpFunc\n",
    "\n",
    "import imblearn\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetFolderName = 'SegmentedData_winLen500_overlap250_'\n",
    "testSize = 0.2\n",
    "useFirstHalf = True\n",
    "selectKbest = 100\n",
    "\n",
    "feature_file = 'X_data_fullFeatures.txt'\n",
    "dataSetFolderName = os.path.join('pre_process', dataSetFolderName)\n",
    "y_fields = ['on_off', 'dyskinesia', 'tremor']\n",
    "labels_folder = 'data'\n",
    "\n",
    "seeds = range(1,3)\n",
    "nworkers = 10\n",
    "\n",
    "parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runClassification(data, labelName, useFirstHalf, selectFeatures, fixClassImbalance = 'none', seed=1, regressionModel=False):\n",
    "    #use only first half of session\n",
    "        \n",
    "    if useFirstHalf:\n",
    "        data = helpFunc.cutSession(data)\n",
    "\n",
    "    # Split to train and test\n",
    "    train, test = helpFunc.splitData(data, testSize=testSize, seed=seed)        \n",
    "\n",
    "    # remove Nans\n",
    "    train = train.dropna(subset=[labelName])\n",
    "    test = test.dropna(subset=[labelName])\n",
    "        \n",
    "    # Get X and y (train and test ) and labels\n",
    "    # reassign indexes of train and test  (which changed after manipulations) to match simple range\n",
    "    train.index = range(len(train))\n",
    "    test.index = range(len(test))\n",
    "\n",
    "    X_train = train.drop(['sessionID'] + y_fields, axis=1)\n",
    "    y_train = train[labelName]\n",
    "    X_test = test.drop(['sessionID'] + y_fields, axis=1)\n",
    "    y_test = test[labelName]\n",
    "\n",
    "    existingLabels = np.unique(np.append(y_train.unique(),y_test.unique()))\n",
    "    labels = {l: str(l) for l in existingLabels}\n",
    "\n",
    "    # Normalization\n",
    "    min_max_scaler = preprocessing.MinMaxScaler(feature_range = (-1,1))\n",
    "    min_max_scaler.fit(pd.concat([X_train, X_test], ignore_index=True))\n",
    "    X_train= min_max_scaler.transform(X_train)\n",
    "    X_test= min_max_scaler.transform(X_test)\n",
    "\n",
    "    # feature selection              \n",
    "    selFeat = selectFeatures.fit(X_train, y_train)\n",
    "    # apply feature selection\n",
    "    X_train = selFeat.transform(X_train)\n",
    "    X_test = selFeat.transform(X_test)\n",
    "    \n",
    "    # handle class imbalance   \n",
    "    if fixClassImbalance != 'none':\n",
    "        if fixClassImbalance=='SMOTEENN':\n",
    "            smote_enn = imblearn.combine.SMOTEENN(random_state=0)\n",
    "            X_train, y_train = smote_enn.fit_resample(X_train, y_train)\n",
    "        elif fixClassImbalance == 'RandomUnderSampler':\n",
    "            rus = imblearn.under_sampling.RandomUnderSampler(random_state=42)\n",
    "            X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "        \n",
    "    # run models\n",
    "    if regressionModel:\n",
    "        clf = RandomForestRegressor(random_state = seed)\n",
    "    else:\n",
    "        clf = RandomForestClassifier(random_state = seed)\n",
    "            \n",
    "    clfRes = helpFunc.runModel(clf, X_train, y_train, X_test, y_test, test, labels, labelName=labelName)         \n",
    "    \n",
    "    return clfRes['mseMean']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSeed(seed):\n",
    "\n",
    "    logging.info(f'Ensemble models: started seed {seed} PID {os.getpid()} PPID {os.getppid()}')\n",
    "    for id in subject_ids:\n",
    "        subjectFolderName = dataSetFolderName + str(id)\n",
    "        data = pd.read_csv(os.path.join(subjectFolderName, feature_file), delim_whitespace=True, header=None, mangle_dupe_cols=True)\n",
    "        # add session number column to the dataframe\n",
    "        data['sessionID'] = pd.read_csv(os.path.join(subjectFolderName ,'sessionIDs.txt'), header=None, squeeze=True)\n",
    "        \n",
    "        # add labels    \n",
    "        for y in y_fields:\n",
    "            y_data = pd.read_csv(os.path.join(subjectFolderName, 'y_'+ y + '.txt'), names=[y], squeeze=True)\n",
    "            data[y] = y_data                    \n",
    "        \n",
    "        logging.info(f'PID {os.getpid()} PPID {os.getppid()} Subject #{id}')\n",
    "\n",
    "        for labelName in y_fields:\n",
    "            # if all values for the current labels are Nans, continue\n",
    "            if data[labelName].isnull().all():\n",
    "                continue\n",
    "\n",
    "            dfRes = eval('all_results_'+labelName)\n",
    "            selectFeatures = SelectKBest(score_func=f_classif, k=selectKbest)  #default\n",
    "\n",
    "            dfRes.loc[[id],[labelName+'_naive']] = scores_naive.loc[id,labelName]\n",
    "\n",
    "            mseBase = runClassification(data, labelName, True, selectFeatures, fixClassImbalance = 'none', seed=seed)    \n",
    "            dfRes.loc[[id],[labelName+'_base']] = mseBase\n",
    "            \n",
    "            selectRegFeatures = SelectKBest(score_func=f_regression, k=selectKbest)  \n",
    "            mseReg = runClassification(data, labelName, True, selectRegFeatures, fixClassImbalance = 'none', seed=seed, regressionModel = True)    \n",
    "            dfRes.loc[[id],[labelName+'_RFregressor']] = mseReg            \n",
    "\n",
    "            mseUseWholeS = runClassification(data, labelName, False, selectFeatures, fixClassImbalance = 'none', seed=seed)        \n",
    "            dfRes.loc[[id],[labelName+'_useWholeS']] = mseUseWholeS\n",
    "\n",
    "            selectFeatures50 = SelectKBest(score_func=f_classif, k=50)\n",
    "            mseKbest50 = runClassification(data, labelName, True, selectFeatures50, fixClassImbalance = 'none', seed=seed)        \n",
    "            dfRes.loc[[id],[labelName+'_kBest50']] = mseKbest50\n",
    "\n",
    "            selectFeaturesRF = SelectFromModel(RandomForestClassifier(random_state = seed))\n",
    "            mseRfFeatueSel = runClassification(data, labelName, True, selectFeaturesRF, fixClassImbalance = 'none', seed=seed)        \n",
    "            dfRes.loc[[id],[labelName+'_rfFeatureSel']] = mseRfFeatueSel\n",
    "            \n",
    "            mseUndersample = runClassification(data, labelName, True, selectFeatures, fixClassImbalance = 'RandomUnderSampler', seed=seed)       \n",
    "            dfRes.loc[[id],[labelName+'_undersample']] = mseUndersample\n",
    "            \n",
    "            # best combination\n",
    "            if (mseBase < mseUseWholeS):\n",
    "                useFirstHalf = True\n",
    "            else:\n",
    "                useFirstHalf = False    \n",
    "\n",
    "            minSelFeatScore = min(mseBase, mseKbest50, mseRfFeatueSel)\n",
    "            if minSelFeatScore == mseBase:\n",
    "                selectFeatures = SelectKBest(score_func=f_classif, k=selectKbest)  #default\n",
    "            elif minSelFeatScore == mseKbest50:\n",
    "                selectFeatures = SelectKBest(score_func=f_classif, k=50)\n",
    "            elif minSelFeatScore == mseRfFeatueSel:\n",
    "                selectFeatures = SelectFromModel(RandomForestClassifier(random_state = seed))   \n",
    "\n",
    "            minClassBalScore = min(mseBase, mseUndersample)\n",
    "            if minClassBalScore == mseBase:\n",
    "                fixClassImbalance = 'none'\n",
    "            elif minClassBalScore == mseUndersample:\n",
    "                fixClassImbalance = 'RandomUnderSampler'\n",
    "        \n",
    "            mseBestComb = runClassification(data, labelName, useFirstHalf, selectFeatures, fixClassImbalance = fixClassImbalance ,seed=seed)\n",
    "            dfRes.loc[[id],[labelName+'_bestCombination']] = mseBestComb                     \n",
    "            \n",
    "    for labelName in y_fields:\n",
    "        dfRes = eval('all_results_'+labelName)\n",
    "        fileName = labelName + '_allResults_seed' + str(seed) + '.csv'\n",
    "        dfRes.to_csv(os.path.join(folder_path, fileName))\n",
    "    \n",
    "    logging.info(f'Ensemble models: finished seed {seed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2020-05-13 10:44:27,159: Running on dataset: real\n",
      "INFO: 2020-05-13 10:44:27,276: Ensemble models: started seed 2 PID 32720 PPID 32639\n",
      "INFO: 2020-05-13 10:44:27,276: Ensemble models: started seed 1 PID 32719 PPID 32639\n",
      "INFO: 2020-05-13 10:44:28,216: PID 32720 PPID 32639 Subject #hbv013\n",
      "INFO: 2020-05-13 10:44:28,241: PID 32719 PPID 32639 Subject #hbv013\n",
      "INFO: 2020-05-13 10:46:38,941: PID 32719 PPID 32639 Subject #hbv038\n",
      "INFO: 2020-05-13 10:46:49,397: PID 32720 PPID 32639 Subject #hbv038\n",
      "INFO: 2020-05-13 10:47:25,000: PID 32719 PPID 32639 Subject #hbv017\n",
      "INFO: 2020-05-13 10:47:36,369: PID 32720 PPID 32639 Subject #hbv017\n",
      "INFO: 2020-05-13 10:48:20,080: PID 32720 PPID 32639 Subject #hbv023\n",
      "INFO: 2020-05-13 10:48:26,541: PID 32719 PPID 32639 Subject #hbv023\n",
      "INFO: 2020-05-13 10:48:49,382: PID 32720 PPID 32639 Subject #hbv051\n",
      "INFO: 2020-05-13 10:48:55,985: PID 32719 PPID 32639 Subject #hbv051\n",
      "INFO: 2020-05-13 10:49:12,980: PID 32720 PPID 32639 Subject #hbv077\n",
      "INFO: 2020-05-13 10:49:22,279: PID 32719 PPID 32639 Subject #hbv077\n",
      "INFO: 2020-05-13 10:49:39,210: PID 32720 PPID 32639 Subject #hbv054\n",
      "INFO: 2020-05-13 10:49:45,310: PID 32719 PPID 32639 Subject #hbv054\n",
      "INFO: 2020-05-13 10:50:41,930: PID 32720 PPID 32639 Subject #hbv014\n",
      "INFO: 2020-05-13 10:50:54,376: PID 32719 PPID 32639 Subject #hbv014\n",
      "INFO: 2020-05-13 10:51:10,703: PID 32720 PPID 32639 Subject #hbv018\n",
      "INFO: 2020-05-13 10:51:23,708: PID 32719 PPID 32639 Subject #hbv018\n",
      "INFO: 2020-05-13 10:51:27,052: PID 32720 PPID 32639 Subject #hbv043\n",
      "INFO: 2020-05-13 10:51:39,404: PID 32719 PPID 32639 Subject #hbv043\n",
      "INFO: 2020-05-13 10:52:15,145: PID 32720 PPID 32639 Subject #hbv022\n",
      "INFO: 2020-05-13 10:52:23,751: PID 32719 PPID 32639 Subject #hbv022\n",
      "INFO: 2020-05-13 10:53:46,784: PID 32720 PPID 32639 Subject #hbv012\n",
      "INFO: 2020-05-13 10:54:05,259: PID 32719 PPID 32639 Subject #hbv012\n",
      "INFO: 2020-05-13 10:54:05,646: Ensemble models: finished seed 2\n",
      "INFO: 2020-05-13 10:54:20,173: Ensemble models: finished seed 1\n",
      "INFO: 2020-05-13 10:54:20,259: Running on dataset: cis\n",
      "INFO: 2020-05-13 10:54:20,358: Ensemble models: started seed 2 PID 585 PPID 32639\n",
      "INFO: 2020-05-13 10:54:20,358: Ensemble models: started seed 1 PID 584 PPID 32639\n",
      "INFO: 2020-05-13 10:54:21,145: PID 584 PPID 32639 Subject #1004\n",
      "INFO: 2020-05-13 10:54:21,154: PID 585 PPID 32639 Subject #1004\n",
      "INFO: 2020-05-13 10:57:25,313: PID 585 PPID 32639 Subject #1006\n",
      "INFO: 2020-05-13 10:57:40,552: PID 584 PPID 32639 Subject #1006\n",
      "INFO: 2020-05-13 10:58:23,501: PID 585 PPID 32639 Subject #1007\n",
      "INFO: 2020-05-13 10:58:43,490: PID 584 PPID 32639 Subject #1007\n",
      "INFO: 2020-05-13 11:19:04,074: PID 585 PPID 32639 Subject #1019\n",
      "INFO: 2020-05-13 11:20:53,523: PID 585 PPID 32639 Subject #1020\n",
      "INFO: 2020-05-13 11:24:51,937: PID 584 PPID 32639 Subject #1019\n",
      "INFO: 2020-05-13 11:26:34,175: PID 584 PPID 32639 Subject #1020\n",
      "INFO: 2020-05-13 11:30:23,946: PID 585 PPID 32639 Subject #1023\n",
      "INFO: 2020-05-13 11:35:26,538: PID 585 PPID 32639 Subject #1032\n",
      "INFO: 2020-05-13 11:35:34,168: PID 584 PPID 32639 Subject #1023\n",
      "INFO: 2020-05-13 11:40:09,890: PID 584 PPID 32639 Subject #1032\n",
      "INFO: 2020-05-13 11:42:39,343: PID 585 PPID 32639 Subject #1034\n",
      "INFO: 2020-05-13 11:44:05,322: PID 585 PPID 32639 Subject #1038\n",
      "INFO: 2020-05-13 11:48:39,717: PID 584 PPID 32639 Subject #1034\n",
      "INFO: 2020-05-13 11:50:12,450: PID 584 PPID 32639 Subject #1038\n",
      "INFO: 2020-05-13 11:56:55,680: PID 585 PPID 32639 Subject #1039\n",
      "INFO: 2020-05-13 12:01:20,764: PID 585 PPID 32639 Subject #1043\n",
      "INFO: 2020-05-13 12:01:22,318: PID 584 PPID 32639 Subject #1039\n",
      "INFO: 2020-05-13 12:02:32,807: PID 585 PPID 32639 Subject #1044\n",
      "INFO: 2020-05-13 12:04:41,370: PID 585 PPID 32639 Subject #1046\n",
      "INFO: 2020-05-13 12:05:41,188: PID 585 PPID 32639 Subject #1048\n",
      "INFO: 2020-05-13 12:06:00,431: PID 584 PPID 32639 Subject #1043\n",
      "INFO: 2020-05-13 12:07:11,509: PID 584 PPID 32639 Subject #1044\n",
      "INFO: 2020-05-13 12:09:26,180: PID 584 PPID 32639 Subject #1046\n",
      "INFO: 2020-05-13 12:09:41,828: PID 585 PPID 32639 Subject #1049\n",
      "INFO: 2020-05-13 12:10:30,262: PID 584 PPID 32639 Subject #1048\n",
      "INFO: 2020-05-13 12:13:12,618: PID 585 PPID 32639 Subject #1051\n",
      "INFO: 2020-05-13 12:14:29,017: PID 584 PPID 32639 Subject #1049\n",
      "INFO: 2020-05-13 12:16:52,041: Ensemble models: finished seed 2\n",
      "INFO: 2020-05-13 12:17:37,378: PID 584 PPID 32639 Subject #1051\n",
      "INFO: 2020-05-13 12:21:10,588: Ensemble models: finished seed 1\n"
     ]
    }
   ],
   "source": [
    "datasets = ['real', 'cis']\n",
    "\n",
    "for dataset in datasets:\n",
    "    logging.info(f'Running on dataset: {dataset}')\n",
    "    label_file = os.path.join(labels_folder, dataset.upper()+'-PD_Training_Data_IDs_Labels.csv')\n",
    "    trainig_labels = pd.read_csv(label_file)\n",
    "    subject_ids = trainig_labels['subject_id'].unique()\n",
    "    subject_count=trainig_labels['subject_id'].value_counts().sort_index()\n",
    "    all_mse_results = pd.DataFrame({  'count': subject_count, 'on_off_naive' : np.nan, 'tremor_naive': np.nan, 'dyskinesia_naive': np.nan})\n",
    "    \n",
    "    # naive scores\n",
    "    scores_naive = pd.DataFrame({'count': subject_count, 'on_off':np.nan,'tremor':np.nan, 'dyskinesia':np.nan})\n",
    "\n",
    "    for id in subject_ids:\n",
    "        subject_indexes = trainig_labels['subject_id'] == id  \n",
    "        for labelName in y_fields:\n",
    "            y_labels = trainig_labels.loc[subject_indexes, labelName]\n",
    "            score = ((y_labels - y_labels.mean())**2).mean()\n",
    "            scores_naive.loc[[id],[labelName]] = score \n",
    "\n",
    "    scores_naive_orig = scores_naive\n",
    "    scores_naive = scores_naive.append(pd.Series(name='Total Score'))\n",
    "    for labelName in y_fields:\n",
    "        sqn = np.sqrt(np.ceil(scores_naive_orig.loc[:, 'count']*0.3).to_numpy(dtype=np.float32))\n",
    "        sqnSum = sqn.sum()\n",
    "        for column in scores_naive_orig.columns[1:]:\n",
    "            score = (sqn * scores_naive_orig.loc[:, column]).sum() / sqnSum\n",
    "            scores_naive.loc['Total Score',column] = score\n",
    "            \n",
    "    # Create results dataframes\n",
    "    all_results_on_off = pd.DataFrame({  'count': subject_count})\n",
    "    all_results_tremor = pd.DataFrame({  'count': subject_count})\n",
    "    all_results_dyskinesia = pd.DataFrame({  'count': subject_count})\n",
    "    for labelName in y_fields:\n",
    "        df = eval('all_results_'+labelName)\n",
    "        df[labelName+'_naive'] = np.nan\n",
    "        df[labelName+'_base'] = np.nan\n",
    "        df[labelName+'_useWholeS'] = np.nan   \n",
    "        df[labelName+'_kBest50'] = np.nan    \n",
    "        df[labelName+'_rfFeatureSel'] = np.nan\n",
    "        df[labelName+'_undersample'] = np.nan\n",
    "        df[labelName+'_bestCombination'] = np.nan\n",
    "        df[labelName+'_RFregressor'] = np.nan# run over seeds\n",
    "\n",
    "    folder_path = os.path.join('features_seeds_summary',dataset, 'Submit_Final')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    if parallel:\n",
    "        with Pool(nworkers) as p:\n",
    "            p.map(processSeed, seeds)\n",
    "    else:\n",
    "        for seed in seeds:\n",
    "            a = processSeed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
