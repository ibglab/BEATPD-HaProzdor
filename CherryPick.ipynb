{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cherry pick\n",
    "Generate the test prediction files using cherry picking from ensembles of feature predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(asctime)s: %(message)s')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import getpass\n",
    "\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "logging.info(\"Numpy version \" + np.__version__)\n",
    "import scipy as sp\n",
    "import scipy.signal as sig\n",
    "logging.info(\"Scipy version \" + sp.__version__)\n",
    "import pandas as pd\n",
    "logging.info(\"Pandas version \" + pd.__version__)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "logging.info(\"Matplotlib version \" + matplotlib.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# For the standard seed use 34\n",
    "np.random.seed (34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['cis','real']\n",
    "score_names = ['tremor', 'dyskinesia', 'on_off']\n",
    "\n",
    "labels_folder = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read label files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels, test_labels, test_data_folder = {}, {}, {}\n",
    "     \n",
    "for dataset in datasets:\n",
    "    test_label_file = os.path.join(labels_folder, dataset.upper()+'-PD_Test_Data_IDs_Labels.csv')\n",
    "    training_label_file = os.path.join(labels_folder, dataset.upper()+'-PD_Training_Data_IDs_Labels.csv')\n",
    "\n",
    "    training_labels[dataset] = pd.read_csv(training_label_file)\n",
    "    logging.info(f'Training {dataset}: Read file \"{training_label_file}\" containing {training_labels[dataset].shape[0]} records')\n",
    "\n",
    "    test_labels[dataset] = pd.read_csv(test_label_file)\n",
    "    logging.info(f'Test {dataset}: Read file \"{test_label_file}\" containing {test_labels[dataset].shape[0]} records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = {}, {}\n",
    "subjects = {}\n",
    "for score in score_names:\n",
    "    field_cols = ['measurement_id', 'subject_id', score]\n",
    "    train_df[score] = pd.concat([training_labels['cis'][field_cols], training_labels['real'][field_cols]]).dropna()\n",
    "    test_df[score] = pd.concat([test_labels['cis'][field_cols], test_labels['real'][field_cols]]).dropna()\n",
    "    subjects[score] = train_df[score]['subject_id'].unique()\n",
    "    logging.info(f'Score: {score}: unique subjects {subjects[score].shape[0]}:\\n{subjects[score]}')\n",
    "    for subject in subjects[score]:\n",
    "        test_df[score].loc[test_df[score]['subject_id']==subject, 'naive_mean'] = train_df[score][train_df[score]['subject_id']==subject][score].mean()\n",
    "        test_df[score].loc[test_df[score]['subject_id']==subject, 'naive_std'] = train_df[score][train_df[score]['subject_id']==subject][score].std()\n",
    "\n",
    "    test_df[score].set_index('measurement_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read features scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in score_names:\n",
    "    feature_results = sorted(glob.glob(f'features/Submit_Final/*{score}_*.csv'))\n",
    "    logging.info(f'Score {score} - number of feature files {len(feature_results)}')\n",
    "    for i, file_name in enumerate(feature_results):\n",
    "        d = pd.read_csv(file_name).set_index('measurement_id')\n",
    "        test_df[score][f'f{i}'] = d['prediction']\n",
    "        idx = test_df[score][f'f{i}'].isna() & ~test_df[score]['naive_mean'].isna()\n",
    "        test_df[score].loc[idx,f'f{i}'] = test_df[score].loc[idx,'naive_mean']      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in score_names:\n",
    "    feature_cols = test_df[score].filter(regex='^f')\n",
    "    test_df[score]['f_mean'] = feature_cols.mean(axis=1)\n",
    "    for i, subject in enumerate(subjects[score]):\n",
    "        for feature_col in feature_cols:\n",
    "            test_df[score].loc[test_df[score]['subject_id']==subject,'a'+feature_col[1:]] = (\n",
    "                test_df[score].loc[test_df[score]['subject_id']==subject, feature_col] - \n",
    "                test_df[score].loc[test_df[score]['subject_id']==subject, feature_col].mean() +\n",
    "                test_df[score].loc[test_df[score]['subject_id']==subject, 'naive_mean'].mean() )\n",
    "    \n",
    "    adjusted_cols = test_df[score].filter(regex='^a')\n",
    "    test_df[score]['a_mean'] = adjusted_cols.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in score_names:\n",
    "    print(test_df[score]['subject_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_subjects = {}\n",
    "naive_subjects['tremor'] = [1007, 1023, 1034, 1046, 'hbv038', 'hbv054', 'hbv012']\n",
    "naive_subjects['dyskinesia'] = [1007, 1023, 1034, 1043, 1044, 1048, 'hbv054', 'hbv018']\n",
    "naive_subjects['on_off'] = [1006, 1044, 'hbv013', 'hbv038', 'hbv051', 'hbv077', 'hbv043']\n",
    "\n",
    "for score in score_names:\n",
    "    for naive_subject in naive_subjects[score]:\n",
    "        test_df[score].loc[test_df[score]['subject_id'] == naive_subject,'a_mean'] =  test_df[score].loc[test_df[score]['subject_id'] == naive_subject,'naive_mean'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['tremor'][['naive_mean','a_mean', 'subject_id']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = {}\n",
    "                    \n",
    "for score in score_names:\n",
    "    f=glob.glob(f'features/BEAT-PD_SC*_{score}_Submit_Final.csv')\n",
    "    submit[score] = pd.read_csv(f[0])\n",
    "    logging.info(f'Submission file {submit[score].shape[0]} records')\n",
    "    s = submit[score].set_index('measurement_id')\n",
    "    s['prediction'] = test_df[score].loc[s.index, 'a_mean']        \n",
    "    \n",
    "    s.loc[s['prediction']<0,'prediction']=0\n",
    "    s.to_csv(f[0].replace('.csv','_Predict.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
